---
title: "Lab-03R"
author: "36-290 -- Statistical Research Methodology"
date: "Week 3 Thursday -- Fall 2021"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

# Preliminaries

## Goal

The goal of this lab is apply K-means and hierarchical clustering.

Note that this lab may have, in your view, relatively few instructions. That's in part because the labs at the back of each chapter in ISLR (the class textbook) provide details about packages and useful "starter code." You should look through (if not work through) these labs either before doing this lab or for extra practice. However, note that the ISRL labs use neither `dplyr` nor `ggplot` (which is fine).

If you are confused: that's what office hours are for.

## Data

We'll begin by importing the stellar data you worked with on Tuesday:
```{r}
rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/DRACO/draco_photometry.Rdata"
load(url(file.path))
df = data.frame(ra,dec,velocity.los,log.g,mag.g,mag.r,mag.i)
rm(file.path,ra,dec,velocity.los,log.g,temperature,mag.u,mag.g,mag.r,mag.i,mag.z,metallicity,signal.noise)
objects()
```

If everything loaded correctly, you should see one variable in your global environment: `df`. `df` is a data frame with 2778 rows and 7 columns. See this [README file](https://github.com/pefreeman/36-290/tree/master/EXAMPLE_DATASETS/DRACO) for a full description of the data and its variables. Note that I have removed `signal.noise`, `metallicity`, `temperature`, and two of the magnitudes from the data frame, to reduce the dimensionality.

# Questions

To answer the questions below, it will help you to refer to Sections 10.3 and 10.5 of ISLR; it might also help you to refer to your previous lab work (and, as always, to Google). 

## Question 1

Filter the data frame such that it only contains values of `dec` &gt; 56, values of `ra` &lt; 264, and values of `velocity.los` between -350 and -250. Mutate the data frame to have g-r and r-i colors, then delete the magnitudes and `velocity.los`. (Pro tip: you can "negatively select" columns by putting minus signs in front of the column names.) Save the resulting data frame as `df.new`.
```{r}

library(magrittr)
library(dplyr)

df.new<- df %>% filter(., dec>56, ra< 264, velocity.los > -350 & velocity.los < -250) %>% mutate(., "g-r"=mag.g-mag.r, "r-i"= mag.r-mag.i ) %>% select(.,-mag.g,-mag.r,-mag.i, -velocity.los)


df.new

#df %>% mutate(.,mass.linear=10^mass) %>% select(.,mass,mass.linear) %>% arrange(.,mass) %>% head(.,3)






```

## Question 2

Use the `kmeans()` function to cluster the data in your data frame. Try different values for K, and finally display results for what *you* would choose as its optimal value. The default for `nstart` is 1; that should be increased to something larger...play with the values for this argument. Display the results using `ggpairs()`, and briefly comment on your interpretation of the results. Pass this argument to `ggpairs()`: `mapping=aes(color=factor(km.out$cluster))`, where `km.out` is the output from K-means, and `cluster` is the number of the cluster to which a datum has been assigned. Hint: if it looks like there are "strips" in `log.g`, you have probably done something wrong. Ruminate on what that might be. Finally ask me if you cannot figure out what might be wrong. Also, note that `kmeans()` utilizes random sampling, so you should absolutely set a random number seed immediately before calling `kmeans()` to enforce reproducibility!
```{r}


library(ggplot2)
library(GGally)

set.seed(101)
km.out <- kmeans(df.new,3,nstart=20)
color = km.out$cluster

ggpairs(df.new,mapping=aes(color=factor(km.out$cluster)), progress=FALSE )



```
```

It seems there are three clusters


```

## Question 3

For your final run of K-means, what are the number of groups and the number of data in each group? Also, what is ratio of the between-cluster sum-of-squares to the total sum-of-squares? (This is a measure of the total variance in the data that is "explained" by clustering. Higher values [closer to 100%] are better, but beware: the larger the value of $K$, the higher the ratio is going to be: you will be getting into the realm of overfitting.) (Hint: `print()` your saved output from `kmeans()`.)

```{r}

print(km.out)




```
```
The number of groups and the number of data in each group?  3 clusters of sizes 162, 687, 369
Ratio of the between-cluster sum-of-squares to the total sum-of-squares: .693 or 69.3%

```

## Question 4

Use the `hclust()` function to build a hierarchical clustering tree for your data frame, and use the basic `plot()` function to display the dendrogram. Examine different forms of linkage: which one makes for the best-looking output? (This should not be confused with: which one gives the best clustering result? Note: there is no "right" answer here; best-looking is in the eye of the statistical consultant.) Despite talking up the dendrogram in class, is this actually useful output here? Why or why not? If your client asked for a dendrogram, what step might you want to consider taking before providing one?


```{r}

hc.out <- hclust(dist(df.new),method="complete")
plot(hc.out)


```



```
Dendrograms are a little hard to read with larger sample sizes so I am not sure if this is as useful in this case. If a client asked for a dendrogram, a step that I might want to consider taking before providing one is considering the different linkage


```

## Question 5

Use the `cutree()` function to map each observation to a cluster, then use `ggpairs()` to display the clusters in a similar manner as above for K-means. Assume the same number of clusters as you did for K-means. Does the output look the same or different from K-means? Is this what you expected? Why or why not? (Hint: if `cluster` is the output from `cutree()`, then `color=factor(cluster)` will properly color each of the points.) Visualizing the output of hierarchical clustering in this manner (rather than using a dendrogram) is better when the sample size is large.
```{r}


cut.tree <- cutree(hc.out, k= 3)

ggpairs(df.new,mapping=aes(color=factor(cut.tree)), progress=FALSE)



```
```
The output looks different from K-means. It is what I expected in the sense that I thought the results would differ from the K-means. However I suppose I didn't expect it to be as drastically different.


```

## Question 6

In your future life as a statistical consultant, you may be faced with a situation where you need to implement new methodologies for which you have no prior knowledge. In short, you have to learn on the fly. And so it will be here. In the notes, I mention Gaussian Mixture Models...so below, I want you to implement a GMM-based analysis using the `ClusterR` package. Assume *two* clusters. Your final goal is to figure out the proportions of the observations that can be confidently placed in either Cluster 1 or Cluster 2 (cluster probabilities &lt;0.05 or &gt;0.95). The placement of the rest of the observations can be considered ambiguous. Issues thinking this through or issues with implementation? Office hours! (Note: you will have to install `ClusterR`.)
```{r}

library(ClusterR)

gmm <- GMM(df.new, 2, dist_mode= "maha_dist",seed_mode = "random_subset", km_iter=10, em_iter=10)

gmm




#The GMM function in the ClusterR package is an R implementation of the Armadillo library class for modeling data as a Gaussian Mixture Model (GMM), under the assumption of diagonal covariance matrices. A number of function parameters can be tuned, among others the gaussian_comps, the dist_mode (eucl_dist, maha_dist), the seed_mode (static_subset, random_subset, static_spread, random_spread), the km_iter and the em_iter (more information about the parameters can be found in the package documentation). Iâ€™ll illustrate the GMM function using the synthetic data dietary_survey_IBS,

#gmm = GMM(dat, 2, dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10,
          
         # em_iter = 10, verbose = F)          

```





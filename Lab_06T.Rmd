---
title: "Lab_06T"
author: "36-290 -- Statistical Research Methodology"
date: "Week 6 Tuesday -- Fall 2021"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

# Preliminaries

## Goal

The goal of this lab is to code and interpret a best subset selection analysis.

## Data

We'll begin by importing galaxy data:
```{r}
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"
load(url(file.path))
df = data.frame(predictors,"y"=response)
rm(file.path,predictors,response)
```

If everything loaded correctly, you should see the variable `df` in your global environment, with 17 measurements each for 3,419 galaxies. The column `y` is a response vector with 3,419 spectroscopically determined redshifts (spectroscopic = "effectively no error in the redshift determination"). I combine the predictors and response into one data frame because this is what the `bestglm()` function, the one you will use below, expects as an input.

To see a full description of the dataset, 
click [here](https://github.com/pefreeman/36-290/tree/master/EXAMPLE_DATASETS/PHOTO_MORPH). The short version of the description is that of the 16 predictor variables, four represent brightness (one magnitude, three colors), and 12 are morphological statistics, i.e., statistics that encode the galaxy's appearance. The question at hand is: which, if any, of the morphological statistics are informative in predicting redshift?

# Questions

To answer the questions below, it will help you to refer to Sections 6.1 of ISLR; it might also help you to refer to your previous lab work (and, as always, to Google).

*Note, however, that you are not going to use the leops package as suggested by ISLR, but rather the bestglm package, which is applicable in both the linear regression and logistic regression regimes.*

## Question 1

Split your data into training and test datasets (keeping in mind that cross-validation is "better" but not necessary in a lab setting).


```{r}

names(df)
#3,419

set.seed(100)
s = sample(nrow(df), 0.7*nrow(df))
df.train = df[s, ]
df.test = df[-s, ]
```

## Question 2

Apply linear regression to your training data, and then compute the mean-squared error using your test data.
```{r}

linreg = lm(y~., data=df.train)
summary(linreg)


test.pred = predict(linreg,newdata=df.test)

predmse = mean((df.test$y - test.pred)^2)
predmse


```

## Question 3

Install the `bestglm` package, if you do not have it installed already. Then load that library and use the function `bestglm()` to perform best subset selection on the training data. Do both AIC and BIC...and for each, display the best model. How many predictor variables are retained in the best models? (Don't include the intercepts.) Do the relative numbers of variables abide by your expectations? Is one model a subset of the other? (Hint: see the documentation for `bestglm()` and look at the part under "Value"...this describes the `R` object that `bestglm()` returns. The best model is included within that object.)
```{r}
library(bestglm)
bg.outBIC = bestglm(df.train,family=gaussian, IC="BIC")

bg.outBIC$BestModel

bg.outAIC= bestglm(df.train,family=gaussian, IC="AIC")
bg.outAIC$BestModel


#"mag.i"  "col.Vi" "col.iJ" "col.JH" "V.G"    "V.M20"  "V.C"    "V.size" "J.G"    "J.M20"  "J.C"    "J.size"
#"H.G"  "H.M20"  "H.C"    "H.size" "y" 



```
```
For BIC:

9 predictor variables retained


For AIC:

10 predictor variables retained


yes it is what I expected because using BIC, I know that each variable selected is important but there is a chance that other important variables might have been left out of the final list whereas using AIC I know that it includes all important variables but there is a chance of including unimportant ones as well. It makes sense that AIC has more predictor variables retained.


from R documentation:

Value
A list with class attribute 'bestglm' and named components:

BestModel	
An lm-object representing the best fitted regression.

Best model is a subset of bestglm


```
## Question 4

The output of `bestglm()` contains, as you saw above, `BestModel`. According to the documentation for `bestglm()`, `BestModel` is "[a]n lm-object representing the best fitted algorithm." That means you can pass it to `predict()` in order to generate predicted response values (where the response is in the `y` column of your data frames). Given this information: generate mean-squared error values for the BIC- and AIC-selected models. Are these values larger or smaller than the value you got for linear regression?

```{r}
#MSE for BIC and AIC model
resp.predBIC = predict(bg.outBIC$BestModel,newdata=df.test)
BICMSE= mean((df.test$y - resp.predBIC)^2)
BICMSE

resp.predAIC = predict(bg.outAIC$BestModel,newdata=df.test)
AICMSE= mean((df.test$y - resp.predAIC)^2)
AICMSE

```

```
MSE value from linear regression: 0.3054208 
MSE value using predicted response values for BIC model: 0.3064319, larger than value from lin reg
MSE value using predicted response values for AIC model: 0.3060815, larger than value from lin reg

```
## Question 5

In Q3, I asked you to output information about the best models for AIC and BIC. Here, choose one of those criteria, and extract the values for that criterion for each $p$ value, where $p$ is the number of retained predictor variables. (Look under `Value` on the `bestglm()` documentation page to see which component of the outputted `R` object contains the information you need.) Then use `ggplot()` to plot the criterion values versus $p$. Zoom in (using `ylim()`) to decrease the dynamic range of the plot and to see more clearly how BIC changes as a function of $p$ near the minimum value. (So as to not hardcode numbers, use, e.g., `min(...)` as the first argument to `ylim`, where you'd replace the `...` with the name of the variable containing the BIC values.)


```{r}


library(ggplot2)
df.bgBIC= data.frame(1:16,bg.outBIC$Subsets$BIC[-1])
names(df.bgBIC) = c("p", "BIC")
ggplot(data=df.bgBIC,mapping=aes(x=p,y=BIC))+
  geom_point() + geom_line()


```

## Question 6
Run the `summary()` function with the best AIC model from Q3. This produces output akin to that of the output from summarizing a linear model (e.g., one output by `lm()`). What is the adjusted $R^2$ value? What does the value imply about the quality of the linear fit with the best subset of variables?
```{r}
summary(bg.outAIC$BestModel)
```
```
adjusted r^2 value is 0.6174 which suggests that the quality of the linear fit with the best subset of variables
```
## Data Part 2

We'll continue by importing data about variable stars:
```{r}
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/TD_CLASS/css_data.Rdata"
load(url(file.path))
tmp = rep("NON-CB",length(response))
tmp[response==1] = "CB"
df = data.frame(predictors,"y"=factor(tmp))
rm(file.path,predictors,response,tmp)


```
A description of these data are [here](https://github.com/pefreeman/36-290/tree/master/EXAMPLE_DATASETS/TD_CLASS). Here, the data frame that is input contains measurements for each of 46,808 stars, of which 30,582 are identified as "contact binaries." The rest are simply *not* contact binaries. Contact binaries are two stars that orbit a common center of mass and which share an envelope of gas. Think of a contact binary as being like a rotating dumbbell (one stellar core at each end), as opposed to the appearance of a single star, which is a simple sphere. Depending on one's vantage point, a rotating dumbbell of gas will have a brightness that varies over the course of one rotation.

The data contain 17 predictor variables that are summary statistics describing the variability of an observed star. The question for now is, which of these 17 are actually informative when we attempt to learn a statistical model relating variability statistics to the type of variable star?

## Question 7
You ultimately would want to repeat Q1, changing the `family` to one that is appropriate for two-class classification, but there's an issue. `bestglm()` in a logistic regression setting limits the number of predictor variables to 15. And given the amount of data we have here, `bestglm()` will be *slow*.So implement either `log_forward()` or `log_backward()` as they are given in the notes to determine the set of variables to keep here. Before doing so, remove `max.slope` from the data frame because some of its values are `Inf`. Don't worry about data splitting here...just run the code and see what you get! Pass all columns but `max.slope` and `y` as the first argument, and the column `y` as the second argument.

```{r}

names(df)

df.no.maxslope = subset(df, select = -c(max.slope))

names(df.no.maxslope)

log_forward = function(df.no.maxslope,y)

{
  var.num = ncol(df.no.maxslope)
  var.keep = aic.keep = c()  
  var.rem = 1:var.num
  
  var = 0  
  while ( var < var.num ) {
    var = var+1
    aic.tmp = rep(0,length(var.rem))
    for ( ii in 1:length(var.rem) ) {
      var.set = c(var.keep,var.rem[ii])
      df = df.no.maxslope[,var.set]
      if ( var == 1 ) df = data.frame(df)
      aic.tmp[ii] = summary(suppressWarnings(glm(y~.,data=df,family=binomial)))$aic
    }
    if ( length(aic.keep) == 0 || min(aic.tmp) < min(aic.keep) ) {
      aic.keep = append(aic.keep,min(aic.tmp))
      w = which.min(aic.tmp)
      var.keep = append(var.keep,var.rem[w])
      var.rem = var.rem[-w]
    } else {
      break
      
    }
  }
  return(sort(names(df.no.maxslope[var.keep])))
}



log_forward





```

